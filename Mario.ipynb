{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym_super_mario_bros==7.3.0 nes_py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B00xlfpLL6AY",
        "outputId": "71d8c751-cb60-4de1-da4d-2ea654020ecd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym_super_mario_bros==7.3.0\n",
            "  Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting nes_py\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes_py) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes_py) (1.26.4)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes_py)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes_py) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes_py) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes_py) (0.0.8)\n",
            "Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nes_py\n",
            "  Building wheel for nes_py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes_py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535719 sha256=5f48d0f26792c2a5402417df02b44f017e568ee8155a4c64673585e85a77a61d\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built nes_py\n",
            "Installing collected packages: pyglet, nes_py, gym_super_mario_bros\n",
            "Successfully installed gym_super_mario_bros-7.3.0 nes_py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rWcw3ojPLtb7"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "from gym.wrappers import GrayScaleObservation,StepAPICompatibility\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Frame Stack Wrapper\n",
        "class CustomVecFrameStack:\n",
        "    def __init__(self, env, n_stack):\n",
        "        self.env = env\n",
        "        self.n_stack = n_stack\n",
        "        self.frames = deque(maxlen=n_stack)\n",
        "        obs_shape = self.env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(obs_shape[0], obs_shape[1], n_stack),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        self.action_space = env.action_space\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(self.n_stack):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_stacked_frames()\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        if done:\n",
        "            obs = self.env.reset()\n",
        "            for _ in range(self.n_stack):\n",
        "                self.frames.append(obs)\n",
        "        return self._get_stacked_frames(), reward, done, info\n",
        "\n",
        "    def _get_stacked_frames(self):\n",
        "        return np.stack(self.frames, axis=-1)"
      ],
      "metadata": {
        "id": "1ssrSRzcL6mF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actor-Critic Network\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        channels, height, width = input_dim\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Conv2d(channels, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.actor = nn.Linear(512, output_dim)\n",
        "        self.critic = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared_features = self.shared(x)\n",
        "        return self.actor(shared_features), self.critic(shared_features)\n",
        "\n",
        "    def get_action_and_value(self, x):\n",
        "        logits, value = self.forward(x)\n",
        "        action_probs = Categorical(logits=logits)\n",
        "        action = action_probs.sample()\n",
        "        return action, action_probs.log_prob(action), value"
      ],
      "metadata": {
        "id": "HDD7aMFnMZcA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO Algorithm\n",
        "class PPO:\n",
        "    def __init__(self, env, input_dim, output_dim, lr=3e-4, gamma=0.99, eps_clip=0.2, gae_lambda=0.95):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.policy = ActorCritic(input_dim, output_dim).cuda()\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    def compute_advantages(self, rewards, values, dones):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for step in reversed(range(len(rewards))):\n",
        "            delta = rewards[step] + self.gamma * (1 - dones[step]) * values[step + 1] - values[step]\n",
        "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae\n",
        "            advantages.insert(0, gae)\n",
        "        return advantages\n",
        "\n",
        "    def update(self, trajectories):\n",
        "        obs, actions, log_probs, rewards, dones, values = trajectories\n",
        "        advantages = self.compute_advantages(rewards, values, dones)\n",
        "        advantages = torch.tensor(advantages).cuda()\n",
        "        values = torch.tensor(values[:-1]).cuda()\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        for _ in range(10):  # PPO epochs\n",
        "            _, new_log_probs, new_values = self.policy.get_action_and_value(obs)\n",
        "            ratios = (new_log_probs - log_probs).exp()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = ((new_values - values) ** 2).mean()\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()"
      ],
      "metadata": {
        "id": "LQLIwxB2MZ0k"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "def train_ppo(env, ppo, num_steps=2048, n_stack=4):\n",
        "    obs_stack = deque(maxlen=n_stack)\n",
        "    obs = env.reset()\n",
        "    for _ in range(n_stack):\n",
        "        obs_stack.append(obs)\n",
        "\n",
        "    trajectories = {\"obs\": [], \"actions\": [], \"log_probs\": [], \"rewards\": [], \"dones\": [], \"values\": []}\n",
        "    for step in range(num_steps):\n",
        "        stacked_obs = np.stack(list(obs_stack), axis=-1)  # Stack frames\n",
        "        obs_tensor = torch.tensor(stacked_obs, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).cuda()\n",
        "\n",
        "        action, log_prob, value = ppo.policy.get_action_and_value(obs_tensor)\n",
        "        new_obs, reward, done, _ = env.step(action.cpu().numpy()[0])\n",
        "\n",
        "        trajectories[\"obs\"].append(obs_tensor)\n",
        "        trajectories[\"actions\"].append(action)\n",
        "        trajectories[\"log_probs\"].append(log_prob)\n",
        "        trajectories[\"rewards\"].append(reward)\n",
        "        trajectories[\"dones\"].append(done)\n",
        "        trajectories[\"values\"].append(value)\n",
        "\n",
        "        obs_stack.append(new_obs)\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "            for _ in range(n_stack):\n",
        "                obs_stack.append(obs)\n",
        "\n",
        "    for key in trajectories:\n",
        "        trajectories[key] = torch.cat(trajectories[key], dim=0).cuda()\n",
        "    ppo.update(trajectories)\n"
      ],
      "metadata": {
        "id": "H7UEFkGaMbnk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment\n",
        "env = gym_super_mario_bros.make(\"SuperMarioBros-v1\")\n",
        "env = StepAPICompatibility(env, new_step_api=True)\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "env = GrayScaleObservation(env, keep_dim=True)\n",
        "env = CustomVecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Define PPO agent\n",
        "ppo = PPO(env, input_dim=(4, 240, 256), output_dim=env.action_space.n)\n",
        "\n",
        "# Train PPO\n",
        "train_ppo(env, ppo, num_steps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "RMTSS0y0Md_q",
        "outputId": "517b84de-46d5-4442-d61a-1263d555f001"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 6 is not equal to len(dims) = 4",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1e861913146c>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train PPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-c2474e7044c5>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(env, ppo, num_steps, n_stack)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstacked_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Stack frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_and_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 6 is not equal to len(dims) = 4"
          ]
        }
      ]
    }
  ]
}